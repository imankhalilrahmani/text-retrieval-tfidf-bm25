{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stories', 'stories/FARNON', 'stories/SRE']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"stories\"\n",
    "alpha = 0.3\n",
    "folders = [x[0] for x in os.walk(str(os.getcwd())+'/'+title+'/')]\n",
    "folders[0] = folders[0][:len(folders[0])-1]\n",
    "folders\n",
    "['stories',\n",
    " 'stories/FARNON',\n",
    " 'stories/SRE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 452\n",
      "0 0\n",
      "15 15\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "c = False\n",
    "\n",
    "for i in folders:\n",
    "    file = open(i+\"/index.html\", 'r')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
    "    file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
    "\n",
    "    if c == False:\n",
    "        file_name = file_name[2:]\n",
    "        c = True\n",
    "        \n",
    "    print(len(file_name), len(file_title))\n",
    "\n",
    "    for j in range(len(file_name)):\n",
    "        dataset.append((str(i) +\"/\"+ str(file_name[j]), file_title[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len (dataset)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_doc(id):\n",
    "    print(dataset[id])\n",
    "    file = open(dataset[id][0], 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\]'\n",
      "C:\\Users\\aliot\\AppData\\Local\\Temp\\ipykernel_22064\\728318708.py:14: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  symbols = \"!#$%&()*+-./:;<=>?@[\\]^_`{|}~\"\n"
     ]
    }
   ],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!#$%&()*+-./:;<=>?@[\\]^_`{|}~\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    " \n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "processed_title = []\n",
    "\n",
    "for i in dataset[:N]:\n",
    "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32350\n",
      "['sharewar', 'trial', 'project', 'freewar', 'need', 'support', 'continu', 'one', 'hundr', 'west', 'fifti', 'three', 'north', 'jim', 'prentic', 'copyright', 'thousand', 'nine', 'nineti', 'brandon']\n",
      "444\n"
     ]
    }
   ],
   "source": [
    "DF = {}\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])\n",
    "   \n",
    "\n",
    "total_vocab_size = len(DF)\n",
    "print(total_vocab_size)\n",
    "total_vocab = [x for x in DF]\n",
    "print(total_vocab[:20])\n",
    "print(DF['one'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    words_count = len(tokens + processed_title[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "\n",
    "tf_idf_title = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_title[i]\n",
    "    counter = Counter(tokens + processed_text[i])\n",
    "    words_count = len(tokens + processed_text[i])\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1)) #numerator is added 1 to avoid negative values\n",
    "        \n",
    "        tf_idf_title[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34742"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tf_idf:\n",
    "    tf_idf[i] *= alpha\n",
    "for i in tf_idf_title:\n",
    "    tf_idf[i] = tf_idf_title[i]\n",
    "len(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in query_weights[:10]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: a lion dont cry\n",
      "\n",
      "['lion', 'dont', 'cri']\n",
      "\n",
      "[266, 288, 267, 265, 87, 0]\n"
     ]
    }
   ],
   "source": [
    "matching_score(10, \"a lion dont cry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('c:\\\\Users\\\\aliot\\\\Desktop\\\\university\\\\data_retrieval\\\\practice\\\\practice_2/stories/lionmosq.txt', 'The Fable of the Lion and the Mosquito')\n",
      "THE LION AND THE MOSQUITO\n",
      "\n",
      "   Once upon a time . . . a tiny mosquito started to buzz round a lion he met.\n",
      "\"Go away!\" grumbled the sleepy lion, smacking his own cheek in an attempt to \n",
      "drive the insect away.\n",
      "   \"Why should I?\" demanded the mosquito. \"You're king of the jungle, not of \n",
      "the air! I'll fly wherever I want and land wherever I please.\" And so saying, \n",
      "he tickled the lion's ear. In the hope of crushing the insect, the lion boxed \n",
      "his own ears, but the mosquito slipped away from the now dazed lion.\n",
      "   \"I don't feel it any more. Either it's squashed or it's gone away.\" But at \n",
      "that very moment, the irritating buzz began again, and the mosquito flew into \n",
      "the lion's nose. Wild with rage, the lion leapt to his hind legs and started \n",
      "to rain punches on his own nose. But the insect, safe inside, refused to \n",
      "budge. With a swollen nose and watery eyes, the lion gave a terrific sneeze, \n",
      "blasting the mosquito out. Angry at being dislodged so abruptly, the mosquito \n",
      "returned to the attack: BUZZ . . . BUZZZ! . . . it whizzed round the lion's \n",
      "head. Large and tough as the lion was, he could not rid himself of his tiny \n",
      "tormenter. This made him angrier still, and he roared fiercely. At the sound \n",
      "of his terrible voice, all the forest creatures fled in fear, but paying no \n",
      "heed to the exhausted lion, the mosquito said triumphantly: \"There you are, \n",
      "king of the jungle! Foiled by a tiny mosquito like me!\" And highly delighted \n",
      "with his victory, off he buzzed. But he did not notice a spider's web hanging \n",
      "close by, and soon he was turning and twisting, trying to escape from the trap\n",
      "set by a large spider. \"Bah!\" said the spider in disgust, as he ate it. \n",
      "\"Another tiny mosquito. Not much to get excited about, but better than \n",
      "nothing. I was hoping for something more substantial...\"\n",
      "   And that's what became of the mosquito that foiled the lion!\n"
     ]
    }
   ],
   "source": [
    "print_doc(266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Matching Score\n",
      "\n",
      "Query: a lion dont cry\n",
      "\n",
      "['lion', 'dont', 'cri']\n",
      "\n",
      "[266, 288, 267, 265, 87]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bm25_score(doc, token, counter, doc_len, N, alpha, k):\n",
    "    tf = counter[token] / doc_len\n",
    "    df = doc_freq(token)\n",
    "    \n",
    "    idf = np.log((N - df + 0.5) / (df + 0.5))\n",
    "    bm25 = idf * (tf * (k + 1)) / (tf + k)\n",
    "    \n",
    "    return bm25 * alpha\n",
    "\n",
    "def matching_score_bm25(query, N, processed_title, processed_text, alpha=1.0, k=1.5):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"BM25 Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for doc in range(N):\n",
    "        counter = Counter(processed_title[doc] + processed_text[doc])\n",
    "        doc_len = len(processed_title[doc] + processed_text[doc])\n",
    "\n",
    "        for token in np.unique(processed_title[doc]):\n",
    "            if token in tokens:\n",
    "                score = bm25_score(doc, token, counter, doc_len, N, alpha, k)\n",
    "                query_weights[doc] = query_weights.get(doc, 0) + score\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "    top_documents = [doc_id for doc_id, _ in query_weights[:10]]\n",
    "    \n",
    "    print(top_documents)\n",
    "\n",
    "matching_score_bm25(\"a lion dont cry\", N, processed_title, processed_text, alpha=1.0, k=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank-bm25 in c:\\users\\aliot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\aliot\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rank-bm25) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(processed_text)\n",
    "\n",
    "def bm25_matching_score_lib(query):\n",
    "    preprocessed_query = preprocess (query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"BM25 Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "\n",
    "    scores = bm25.get_scores(tokens)\n",
    "\n",
    "    relevant_documents = sorted(range(len(scores)), key=lambda i: scores[i],\n",
    "    reverse=True) [:10]\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(relevant_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: lions tiger\n",
      "\n",
      "['lion', 'tiger']\n",
      "\n",
      "[0]\n",
      "BM25 Matching Score\n",
      "\n",
      "Query: lions tiger\n",
      "\n",
      "['lion', 'tiger']\n",
      "\n",
      "\n",
      "[363, 257, 26, 266, 27, 288, 267, 150, 434, 265]\n"
     ]
    }
   ],
   "source": [
    "matching_score(10, \"lions tiger\")\n",
    "bm25_matching_score_lib(\"lions tiger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('c:\\\\Users\\\\aliot\\\\Desktop\\\\university\\\\data_retrieval\\\\practice\\\\practice_2/stories/lionmosq.txt', 'The Fable of the Lion and the Mosquito')\n",
      "THE LION AND THE MOSQUITO\n",
      "\n",
      "   Once upon a time . . . a tiny mosquito started to buzz round a lion he met.\n",
      "\"Go away!\" grumbled the sleepy lion, smacking his own cheek in an attempt to \n",
      "drive the insect away.\n",
      "   \"Why should I?\" demanded the mosquito. \"You're king of the jungle, not of \n",
      "the air! I'll fly wherever I want and land wherever I please.\" And so saying, \n",
      "he tickled the lion's ear. In the hope of crushing the insect, the lion boxed \n",
      "his own ears, but the mosquito slipped away from the now dazed lion.\n",
      "   \"I don't feel it any more. Either it's squashed or it's gone away.\" But at \n",
      "that very moment, the irritating buzz began again, and the mosquito flew into \n",
      "the lion's nose. Wild with rage, the lion leapt to his hind legs and started \n",
      "to rain punches on his own nose. But the insect, safe inside, refused to \n",
      "budge. With a swollen nose and watery eyes, the lion gave a terrific sneeze, \n",
      "blasting the mosquito out. Angry at being dislodged so abruptly, the mosquito \n",
      "returned to the attack: BUZZ . . . BUZZZ! . . . it whizzed round the lion's \n",
      "head. Large and tough as the lion was, he could not rid himself of his tiny \n",
      "tormenter. This made him angrier still, and he roared fiercely. At the sound \n",
      "of his terrible voice, all the forest creatures fled in fear, but paying no \n",
      "heed to the exhausted lion, the mosquito said triumphantly: \"There you are, \n",
      "king of the jungle! Foiled by a tiny mosquito like me!\" And highly delighted \n",
      "with his victory, off he buzzed. But he did not notice a spider's web hanging \n",
      "close by, and soon he was turning and twisting, trying to escape from the trap\n",
      "set by a large spider. \"Bah!\" said the spider in disgust, as he ate it. \n",
      "\"Another tiny mosquito. Not much to get excited about, but better than \n",
      "nothing. I was hoping for something more substantial...\"\n",
      "   And that's what became of the mosquito that foiled the lion!\n"
     ]
    }
   ],
   "source": [
    "print_doc(266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('c:\\\\Users\\\\aliot\\\\Desktop\\\\university\\\\data_retrieval\\\\practice\\\\practice_2/stories/redragon.txt', 'The Fable of the Red Dragon')\n",
      "THE RED DRAGON\n",
      "\n",
      "   Once there was a time, thousands of years ago, when animals were not the \n",
      "same as they are now. Except for a few like the lion, the tiger and the \n",
      "butterflies, they all looked alike. All were more or less the same height, \n",
      "everyone had four legs and it wasn't easy to tell which was which, even though\n",
      "the elephant did weigh more than the hyena, and the hippo more than the \n",
      "gazelle. One day, while all the animals were relaxing in a field, along came a\n",
      "red dragon, out of breath, crying,\n",
      "   \"We're in danger, folks! The world is about to come to an end!\"\n",
      "   \"How do you know?\" everyone asked. The dragon replied,\n",
      "   \"I read it in the stars. We must escape!\"\n",
      "   \"But where can we go?\" they asked him.\n",
      "   \"To another world,\" he replied. \"I'll take you there. I can fly and I'll \n",
      "take you to a planet that is safer than this one.\" Frightened, as they were, \n",
      "all the animals climbed on to the dragon's back.\n",
      "   With a bored look, the lion said, \"I'm not scared of anything, so I'll just\n",
      "stay here on Earth.\" The others, however, were fighting to get on the dragon's\n",
      "back.\n",
      "   \"Don't push, you behind!\" shouted the crocodile.\n",
      "   \"Hey, move that paw!\" It was just like people today pushing and shoving to \n",
      "get onto an overcrowded train. At last the dragon cried,\n",
      "   \"Ready! Off we go,\" and started to run for takeoff. The first and the \n",
      "second runs weren't fast enough, but at the third try he finally got off the \n",
      "ground, flapping his wings and waving his tail.\n",
      "   \"Not so fast!\" shouted somebody, and another voice yelled: \"Faster, or we \n",
      "will end up in the trees!\" The dragon replied,\n",
      "   \"Oh, bother! I'm doing the best I can. Why don't you lot keep still, for \n",
      "once.\" The fact was that because they were frightened, they did everything but\n",
      "keep still, and so, after a while, the poor red dragon, now very tired, simply\n",
      "could not flap his wings any longer . . . and crashed on a lovely green meadow.\n",
      "   All the animals shrieked with terror. Nobody lost his life . . but the \n",
      "snake lost his legs and slithered away through the grass. The rhino bumped his\n",
      "head and grew a horn. All the elephant's teeth fell out, except for two which \n",
      "became very long. The giraffe sprained his neck and it grew to a great length.\n",
      "The hippo rolled about so much he became nearly round, ended up in a pond and \n",
      "didn't come out, he was too ashamed to be seen . . . Well, in that fall, all \n",
      "the animals took on a different appearance and became what they are today. And\n",
      "when the lion saw them, what he said was:\n",
      "   \"Oh, how funny you look!\"\n"
     ]
    }
   ],
   "source": [
    "print_doc(363)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a43caa0e1de43b7e0007c6f6c792efdd908e8fa3d1755c007de1cb87ce90b3ba"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
